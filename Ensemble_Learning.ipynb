{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Can we use Bagging for regression problems?**\n",
        "\n",
        "Yes. Bagging can be used for regression. It trains multiple regressors on bootstrapped samples and averages their predictions, reducing variance and improving stability.\n",
        "\n",
        "**2. Difference between multiple model training and single model training**\n",
        "\n",
        "| Single Model        | Multiple (Ensemble) Models |\n",
        "|--------------------|---------|\n",
        "| One model trained      | Many models trained    |\n",
        "| High variance | Reduced variance    |\n",
        "| More prone to overfitting | More robust    |\n",
        "| Less accurate | Higher accuracy    |\n",
        "\n",
        "**3. Feature randomness in Random Forest**\n",
        "\n",
        "Random Forest selects a random subset of features at every split. This makes trees diverse and prevents dominance of strong predictors.\n",
        "\n",
        "**4. What is OOB Score?**\n",
        "\n",
        "Out-of-Bag (OOB) score uses data not selected in bootstrap samples to evaluate model performance without a separate test set.\n",
        "\n",
        "**5. How to measure feature importance in Random Forest?**\n",
        "\n",
        "By calculating how much each feature reduces impurity (Gini or MSE) across all trees.\n",
        "\n",
        "**6. Working principle of Bagging Classifier**\n",
        "\n",
        "Create multiple bootstrap samples\n",
        "\n",
        "Train one model on each\n",
        "\n",
        "Combine predictions using majority voting\n",
        "\n",
        "**7. How to evaluate Bagging Classifier?**\n",
        "\n",
        "Using:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision, Recall, F1-score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "**8. How does a Bagging Regressor work?**\n",
        "\n",
        "It trains multiple regressors on bootstrapped datasets and averages their outputs.\n",
        "\n",
        "**9. Main advantage of ensemble techniques**\n",
        "\n",
        "Higher accuracy and reduced overfitting.\n",
        "\n",
        "**10. Main challenge of ensemble methods**\n",
        "\n",
        "High computational cost and reduced interpretability.\n",
        "\n",
        "**11. Key idea behind ensemble techniques**\n",
        "\n",
        "Combining multiple weak learners creates a strong learner.\n",
        "\n",
        "**12. What is Random Forest Classifier?**\n",
        "\n",
        "An ensemble of decision trees built using bootstrap samples and feature randomness.\n",
        "\n",
        "**13. Main types of ensemble techniques**\n",
        "\n",
        "Bagging\n",
        "\n",
        "Boosting\n",
        "\n",
        "Stacking\n",
        "\n",
        "**14. What is ensemble learning?**\n",
        "\n",
        "A technique that combines multiple models to improve performance.\n",
        "\n",
        "**15. When should we avoid ensembles?**\n",
        "\n",
        "When:\n",
        "\n",
        "Data is very small\n",
        "\n",
        "Real-time prediction is required\n",
        "\n",
        "Interpretability is important\n",
        "\n",
        "**16. How does Bagging reduce overfitting?**\n",
        "\n",
        "By averaging multiple high-variance models, variance decreases.\n",
        "\n",
        "**17. Why is Random Forest better than a single tree?**\n",
        "\n",
        "It reduces overfitting and improves accuracy through averaging.\n",
        "\n",
        "**18. Role of bootstrap sampling**\n",
        "\n",
        "Creates diverse datasets for training different models.\n",
        "\n",
        "**19. Real-world applications**\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Stock prediction\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "**20. Difference between Bagging and Boosting**\n",
        "\n",
        "| Bagging | Boosting |\n",
        "|----------------|----------|\n",
        "| Parallel training   | Sequential training |\n",
        "| Reduces variance | Reduces bias |\n",
        "| Equal weight | Weighted samples |"
      ],
      "metadata": {
        "id": "VeKDfKkH1zTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udRTzjLR1xcc",
        "outputId": "951e3433-1fb7-41a2-e5ad-fd10bc055c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21. Bagging Classifier\n",
            "Accuracy: 0.9298245614035088\n",
            "22. Bagging Regressor\n",
            "MSE: 1233.70220691645\n",
            "23. Random Forest Classifier – Feature Importance\n",
            "Random Forest Classifier Accuracy: 0.9707602339181286\n",
            "\n",
            "Q23 – Random Forest Classifier Feature Importance\n",
            "            Feature Name  Importance\n",
            "7    mean concave points    0.141934\n",
            "27  worst concave points    0.127136\n",
            "23            worst area    0.118217\n",
            "6         mean concavity    0.080557\n",
            "20          worst radius    0.077975\n",
            "24. RF vs Decision Tree\n",
            "DT MSE: 3698.253646146087\n",
            "RF MSE: 1273.105982683196\n",
            "25. OOB Score\n",
            "OOB Score: 0.9547738693467337\n",
            "26. Bagging with SVM\n",
            "0.9473684210526315\n",
            "27. RF with different trees\n",
            "10 0.9649122807017544\n",
            "50 0.9707602339181286\n",
            "100 0.9707602339181286\n",
            "28. Bagging with Logistic Regression (AUC)\n",
            "Q28 – AUC Score: 0.9976484420928865\n",
            "29. RF Regressor Feature Importance\n",
            "\n",
            "Random Forest Regressor MSE: 476.95242474941597\n",
            "\n",
            "Q29 – Random Forest Regressor Feature Importance\n",
            "  Feature Index  Importance\n",
            "1     Feature 1    0.545886\n",
            "0     Feature 0    0.171348\n",
            "3     Feature 3    0.144629\n",
            "4     Feature 4    0.079264\n",
            "2     Feature 2    0.058874\n",
            "30. Bagging vs Random Forest\n",
            "Bagging: 0.9590643274853801\n",
            "Random Forest: 0.9649122807017544\n"
          ]
        }
      ],
      "source": [
        "#All tasks are from page-1 of your PDF Ensemble_Learning\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "#21. Bagging Classifier\n",
        "print(\"21. Bagging Classifier\")\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
        "model = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50)\n",
        "model.fit(X_train,y_train)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
        "\n",
        "#22. Bagging Regressor\n",
        "print(\"22. Bagging Regressor\")\n",
        "X,y = make_regression(n_samples=1000,n_features=5)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
        "model = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50)\n",
        "model.fit(X_train,y_train)\n",
        "print(\"MSE:\", mean_squared_error(y_test, model.predict(X_test)))\n",
        "\n",
        "#23. Random Forest Classifier – Feature Importance\n",
        "print(\"23. Random Forest Classifier – Feature Importance\")\n",
        "# Load classification dataset (Breast Cancer)\n",
        "data = load_breast_cancer()\n",
        "X_cls = data.data\n",
        "y_cls = data.target\n",
        "feature_names = load_breast_cancer().feature_names\n",
        "\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "    X_cls, y_cls, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "y_pred_cls = rf_classifier.predict(X_test_cls)\n",
        "print(\"Random Forest Classifier Accuracy:\", accuracy_score(y_test_cls, y_pred_cls))\n",
        "\n",
        "# Feature Importance for Classifier\n",
        "clf_importance = pd.DataFrame({\n",
        "    \"Feature Name\": feature_names,\n",
        "    \"Importance\": rf_classifier.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nQ23 – Random Forest Classifier Feature Importance\")\n",
        "print(clf_importance.head())\n",
        "\n",
        "#24. RF vs Decision Tree\n",
        "print(\"24. RF vs Decision Tree\")\n",
        "dt = DecisionTreeRegressor()\n",
        "rf = RandomForestRegressor()\n",
        "dt.fit(X_train,y_train)\n",
        "rf.fit(X_train,y_train)\n",
        "print(\"DT MSE:\", mean_squared_error(y_test, dt.predict(X_test)))\n",
        "print(\"RF MSE:\", mean_squared_error(y_test, rf.predict(X_test)))\n",
        "\n",
        "#25. OOB Score\n",
        "print(\"25. OOB Score\")\n",
        "rf = RandomForestClassifier(n_estimators=100,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42)\n",
        "rf.fit(X_train_cls, y_train_cls)\n",
        "print(\"OOB Score:\", rf.oob_score_)\n",
        "\n",
        "#26. Bagging with SVM\n",
        "print(\"26. Bagging with SVM\")\n",
        "model = BaggingClassifier(SVC(), n_estimators=10)\n",
        "model.fit(X_train_cls, y_train_cls)\n",
        "print(accuracy_score(y_test_cls, model.predict(X_test_cls)))\n",
        "\n",
        "#27. RF with different trees\n",
        "print(\"27. RF with different trees\")\n",
        "for n in [10,50,100]:\n",
        "  rf = RandomForestClassifier(n_estimators=n)\n",
        "  rf.fit(X_train_cls, y_train_cls)\n",
        "  print(n, accuracy_score(y_test_cls, rf.predict(X_test_cls)))\n",
        "\n",
        "#28. Bagging with Logistic Regression (AUC)\n",
        "print(\"28. Bagging with Logistic Regression (AUC)\")\n",
        "log_reg = LogisticRegression(max_iter=5000)\n",
        "\n",
        "bag_lr = BaggingClassifier(\n",
        "    estimator=log_reg,\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_lr.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "y_pred_proba = bag_lr.predict_proba(X_test_cls)[:,1]\n",
        "\n",
        "print(\"Q28 – AUC Score:\", roc_auc_score(y_test_cls, y_pred_proba))\n",
        "\n",
        "#29. RF Regressor Feature Importance\n",
        "print(\"29. RF Regressor Feature Importance\")\n",
        "# Create regression dataset\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "y_pred_reg = rf_regressor.predict(X_test_reg)\n",
        "print(\"\\nRandom Forest Regressor MSE:\", mean_squared_error(y_test_reg, y_pred_reg))\n",
        "\n",
        "# Feature Importance for Regressor\n",
        "reg_importance = pd.DataFrame({\n",
        "    \"Feature Index\": [f\"Feature {i}\" for i in range(X_reg.shape[1])],\n",
        "    \"Importance\": rf_regressor.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nQ29 – Random Forest Regressor Feature Importance\")\n",
        "print(reg_importance)\n",
        "\n",
        "#30. Bagging vs Random Forest\n",
        "print(\"30. Bagging vs Random Forest\")\n",
        "bag = BaggingClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "bag.fit(X_train_cls,y_train_cls)\n",
        "rf.fit(X_train_cls,y_train_cls)\n",
        "print(\"Bagging:\", accuracy_score(y_test_cls, bag.predict(X_test_cls)))\n",
        "print(\"Random Forest:\", accuracy_score(y_test_cls, rf.predict(X_test_cls)))"
      ]
    }
  ]
}